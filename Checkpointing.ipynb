{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Checkpointing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM0Ias6zBUtJ/tvpMFVB81F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azfarkhoja305/GANs/blob/checkpoint/Checkpointing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMm3ADuPN7W3"
      },
      "source": [
        "## Created this notebook for colab. \n",
        "Will require chnages if run locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbuHjgULOF3W"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSPym49AaZM4"
      },
      "source": [
        "from pathlib import Path\n",
        "import pdb\n",
        "import sys\n",
        "import re\n",
        "\n",
        "Path.ls = lambda x: list(x.iterdir())"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zduq4XepadbM"
      },
      "source": [
        "gdrive = Path('drive/MyDrive')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7WQI-LzZuqO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqqfDqi5UH2m"
      },
      "source": [
        "!git clone https://github.com/azfarkhoja305/GANs.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3tQgeqUxgY"
      },
      "source": [
        "if Path('./GANs').exists():\n",
        "    sys.path.insert(0,'./GANs')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H_pACzAVZqf"
      },
      "source": [
        "from utils.utils import check_gpu"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h39eeB7gVk6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ef9f55-2ca8-4b4e-9c81-e863df1e1a87"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSvWsEDGVj6H",
        "outputId": "ee85a092-0eae-45a2-9efd-085a773cc069"
      },
      "source": [
        "device = check_gpu()\n",
        "print(f'Using device: {device}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGMNDiBv3NPx"
      },
      "source": [
        "class Dummy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(100,2)\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3g8cnj9Vxjn"
      },
      "source": [
        "gen = Dummy().to(device)\n",
        "critic = Dummy().to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwEDFjibY3oE"
      },
      "source": [
        "# hyper params\n",
        "lr = 3e-4\n",
        "gen_opt = optim.AdamW(Gen.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "critic_opt = optim.AdamW(Dis.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "num_epochs=20"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSIr6W5naHRT"
      },
      "source": [
        "# store loss statistics\n",
        "loss_logs = {'train_loss': [], 'valid_loss': []}"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcwNsK9EaJwx"
      },
      "source": [
        "# Create a required checkpoint instance. \n",
        "# If does not exists, Checkpoint class will create one.\n",
        "ckp_folder = gdrive/'temporary_checkpoint'"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyxRk-l2ajR5"
      },
      "source": [
        "class Checkpoint():\n",
        "    \"\"\" Saves checkpoints at required epochs. Additionally \n",
        "        automatically picks up the latest checkpoint if the folder already exists.\n",
        "        Can also load the checkpoint given the file \"\"\"\n",
        "    def __init__(self, ckp_folder, max_epochs, num_ckps, start_after=0.5):\n",
        "        \"\"\" Start checkpointing after `start_after*max_epoch`. \n",
        "            Like start after 50% of max_epochs completed and divides the number of\n",
        "            checkpoints equally. \"\"\"\n",
        "        self.ckp_folder = ckp_folder\n",
        "        self.max_epochs = max_epochs\n",
        "        self.num_ckps = num_ckps\n",
        "        self.ckp_epochs = np.linspace(start_after*max_epochs, max_epochs, \n",
        "                                      num_ckps, dtype=np.int).tolist() \n",
        "        if isinstance(self.ckp_folder, str):\n",
        "            self.ckp_folder = Path(self.ckp_folder)\n",
        "    \n",
        "    def check_if_exists(self, generator, critic, gen_opt, critic_opt ):\n",
        "        if not self.ckp_folder.exists():\n",
        "            self.ckp_folder.mkdir(parents=True)\n",
        "            return generator, critic, gen_opt, critic_opt, 0, None\n",
        "\n",
        "        ckp_files = [file for file in self.ckp_folder.ls() if file.suffix in ['.pth','.pt']]\n",
        "        if not ckp_files:\n",
        "            return  generator, critic, gen_opt, critic_opt, 0, None\n",
        "        print(\"Checkpoint folder with checkpoints already exists. Searching for the latest.\")\n",
        "        # finding latest (NOT best) checkpoint to resume train\n",
        "        numbers = [int(re.search(r'\\d+', name.stem).group()) for name in ckp_files]\n",
        "        idx = max(enumerate(numbers), key=lambda x: x[1])[0]\n",
        "        return self.load_checkpoint(ckp_files[idx], generator, critic, gen_opt, critic_opt)\n",
        "\n",
        "    def at_epoch_end(self, generator, critic, gen_opt, critic_opt, epoch, loss_logs):\n",
        "        if epoch in self.ckp_epochs:\n",
        "            self.save_checkpoint(self.ckp_folder/f'GanModel_{epoch}.pth',\n",
        "                                 generator, critic, gen_opt, critic_opt,\n",
        "                                 epoch, loss_logs)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_checkpoint(ckp_path, generator, critic, gen_opt=None, critic_opt=None):\n",
        "        assert isinstance(generator, nn.Module), f'Generator is not nn.Module'\n",
        "        assert isinstance(critic, nn.Module), f'Discriminator is not nn.Module'\n",
        "        if isinstance(ckp_path, str): \n",
        "            ckp_path = Path(ckp_path)\n",
        "        assert ckp_path.exists(), f'Checkpoint File: {str(ckp_path)} does not exist'\n",
        "        print(f\"=> Loading checkpoint: {ckp_path}\")\n",
        "        ckp = torch.load(ckp_path)\n",
        "        generator.load_state_dict(ckp['generator_state_dict'])\n",
        "        critic.load_state_dict(ckp['critic_state_dict'])\n",
        "        if gen_opt is not None and ckp['gen_optim_state_dict'] is not None:\n",
        "            gen_opt.load_state_dict(ckp['gen_optim_state_dict'])\n",
        "        if critic_opt is not None and ckp['critic_optim_state_dict'] is not None:\n",
        "            critic_opt.load_state_dict(ckp['critic_optim_state_dict'])\n",
        "\n",
        "        epoch_complete = ckp['epoch']\n",
        "        loss_logs = ckp['loss_logs']\n",
        "        return generator, critic, gen_opt, critic_opt, epoch_complete+1, loss_logs\n",
        "    \n",
        "    @staticmethod\n",
        "    def save_checkpoint(file_path, generator, critic, gen_opt=None, \n",
        "                        critic_opt=None, epoch=-1, loss_logs=None):\n",
        "        assert not file_path.is_dir(), f\"`file_path` cannot be a dir, Needs to be dir/file_name\"\n",
        "        ckp_suffix = ['.pth','.pt']\n",
        "        assert file_path.suffix in ckp_suffix, f'{file_path.name} is not in checkpoint file format'\n",
        "        assert isinstance(generator, nn.Module), f'Generator is not nn.Module'\n",
        "        assert isinstance(critic, nn.Module), f'Discriminator is not nn.Module'\n",
        "        print(f\"=> Saving Checkpoint with name `{file_path.name}`\")\n",
        "        gen_opt_dict = gen_opt.state_dict() if gen_opt is not None else None\n",
        "        critic_opt_dict = critic_opt.state_dict() if critic_opt is not None else None\n",
        "        torch.save({\n",
        "                    'generator_state_dict': generator.state_dict(),\n",
        "                    'critic_state_dict': critic.state_dict(),\n",
        "                    'gen_optim_state_dict':  gen_opt_dict,\n",
        "                    'critic_optim_state_dict': critic_opt_dict,\n",
        "                    'epoch': epoch,\n",
        "                    'loss_logs': loss_logs\n",
        "                    }, file_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def delete_checkpoint(file_path):\n",
        "        if isinstance(file_path, str): \n",
        "            file_path = Path(file_path)\n",
        "        ckp_suffix = ['.pth','.pt']\n",
        "        assert file_path.suffix in ckp_suffix, f'{file_path.name} is not in checkpoint file format'\n",
        "        assert file_path.exists(), f\"`file_path`: {str(file_path)} not found\" \n",
        "        print(f\"Deleting {str(file_path)}\")\n",
        "        file_path.unlink()\n",
        "    \n",
        "    def find_best_ckp():\n",
        "        \"\"\" Calculate the metric for each checkpoint and return best\"\"\"\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_OLbzeD90g2"
      },
      "source": [
        "# Before starting training, instantiate the Checkpoint class\n",
        "# start checkpointing after 50 % of max_epochs are completed\n",
        "ckp_class = Checkpoint(ckp_folder, max_epochs=20, num_ckps=5, start_after=0.5)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qamo61HUQQwa",
        "outputId": "923a3fc3-d027-4751-840b-63a35722773c"
      },
      "source": [
        "# check if any existing checkpoint exists, none found hence start_epoch is 0.\n",
        "# Optimizer states also get saved\n",
        "gen, critic, gen_opt, critic_opt, start_epoch, old_logs = \\\n",
        "                        ckp_class.check_if_exists(gen, critic, gen_opt, critic_opt)\n",
        "\n",
        "loss_logs = old_logs or loss_logs\n",
        "start_epoch, loss_logs"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, {'train_loss': [], 'valid_loss': []})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7VOBpWdGmlh",
        "outputId": "8d635b59-296f-4980-9923-b2fb46fa920e"
      },
      "source": [
        "# these are the epochs where checkpoint will be stored.\n",
        "# The range [start_after*max_epochs, max_epochs] get equally divided\n",
        "ckp_class.ckp_epochs"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 12, 15, 17, 20]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-4nMnbGkIAy"
      },
      "source": [
        "# at the end of each epoch of training, do this\n",
        "# if epoch is in `ckp_class.ckp_epochs` (above) it will save the checkpoints.\n",
        "# Otherwise does nothing, like in this example\n",
        "ckp_class.at_epoch_end(gen, critic, gen_opt, critic_opt, epoch=5, loss_logs=loss_logs)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pkKgnAGH3V9",
        "outputId": "29d806b0-254a-4b94-8188-235ba5ec1c68"
      },
      "source": [
        "# Since this epoch is in `ckp_class.ckp_epochs`, it will save a checkpoint.\n",
        "# It gets named as `GanModel_{epoch}.pth' \n",
        "ckp_class.at_epoch_end(gen, critic, gen_opt, critic_opt, epoch=10, loss_logs=loss_logs)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> Saving Checkpoint with name `GanModel_10.pth`\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zJ8KUheK2vS",
        "outputId": "71b1ed92-a779-434d-cbae-54e8cacd8c7a"
      },
      "source": [
        "# Saving one more \n",
        "ckp_class.at_epoch_end(gen, critic, gen_opt, critic_opt, epoch=15, loss_logs=loss_logs)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> Saving Checkpoint with name `GanModel_15.pth`\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9fOy6jWGWxe",
        "outputId": "b9efae41-45c7-4d8a-dc53-944d7c67b768"
      },
      "source": [
        "# Now in the future say training crashes or stops, this will automatically \n",
        "# pick up the latest checkpoint, no extra code or setting required\n",
        "# `start_epoch` is completed epochs + 1\n",
        "gen, critic, gen_opt, critic_opt, start_epoch, old_logs= \\\n",
        "                    ckp_class.check_if_exists(gen, critic, gen_opt, critic_opt)\n",
        "\n",
        "start_epoch"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint folder with checkpoints already exists. Searching for the latest.\n",
            "=> Loading checkpoint: drive/MyDrive/temporary_checkpoint/GanModel_15.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4u4Dy6jRKQ5",
        "outputId": "6cf3391d-1006-429f-91c1-03d3ccb41ef8"
      },
      "source": [
        "# We can also manually save a model with any name we like.\n",
        "# Need to directly use class name for this, optimizers are not necessary\n",
        "Checkpoint.save_checkpoint(ckp_folder/'transgan_50.pth', gen, critic)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> Saving Checkpoint with name `transgan_50.pth`\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_jdmjCxS38a",
        "outputId": "cbfa67f0-2697-4baf-e001-5d64f99bf40a"
      },
      "source": [
        "# Looking inside the checkpoint folder\n",
        "ckp_folder.ls()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('drive/MyDrive/temporary_checkpoint/GanModel_10.pth'),\n",
              " PosixPath('drive/MyDrive/temporary_checkpoint/GanModel_15.pth'),\n",
              " PosixPath('drive/MyDrive/temporary_checkpoint/transgan_50.pth')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNk2qmCAQflC",
        "outputId": "11a3bf1f-d583-4997-eb5d-a489749b923a"
      },
      "source": [
        "# Deleting checkpoints \n",
        "Checkpoint.delete_checkpoint(ckp_folder/'GanModel_10.pth')\n",
        "Checkpoint.delete_checkpoint(ckp_folder/'GanModel_15.pth')\n",
        "Checkpoint.delete_checkpoint(ckp_folder/'transgan_50.pth')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deleting drive/MyDrive/temporary_checkpoint/GanModel_10.pth\n",
            "Deleting drive/MyDrive/temporary_checkpoint/GanModel_15.pth\n",
            "Deleting drive/MyDrive/temporary_checkpoint/transgan_50.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8neWjmXLDcr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}