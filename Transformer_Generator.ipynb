{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Generator",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwaVe8UIslMhwUjX32KNu9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azfarkhoja305/GANs/blob/transformer_generator/Transformer_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYWGNU4Vjd0e"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import functools \n",
        "import pdb\n",
        "\n",
        "Path.ls = lambda x: list(x.iterdir())"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgtfey48j1VS"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation, rc\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torchsummary import summary"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3ozO59bj32p"
      },
      "source": [
        "!git clone -b transformer_generator https://github.com/azfarkhoja305/GANs.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mFwWmmmj5v4"
      },
      "source": [
        "if Path('./GANs').exists():\n",
        "    sys.path.insert(0,'./GANs')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PaOLoyy4qok"
      },
      "source": [
        "from utils.utils import check_gpu\n",
        "from utils.trunc_normal import trunc_normal_\n",
        "from utils.layers import DropPath, gelu, LinearReshape, Block, PixelUpsample, To_RGB"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2Geq5XQ4sMz"
      },
      "source": [
        "## Attention Mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQJAr3oFIFwc"
      },
      "source": [
        "# TODO: Is this the one in the paper? \n",
        "# Doesn't seem so\n",
        "def get_attn_mask(N, w):\n",
        "    mask = torch.zeros(1, 1, N, N)\n",
        "    for i in range(N):\n",
        "        if i <= w:\n",
        "            mask[:, :, i, 0:i+w+1] = 1\n",
        "        elif N - i <= w:\n",
        "            mask[:, :, i, i-w:N] = 1\n",
        "        else:\n",
        "            mask[:, :, i, i:i+w+1] = 1\n",
        "            mask[:, :, i, i-w:i] = 1\n",
        "    return mask"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "KAPzOImrMR59",
        "outputId": "5f79e4b4-d6c8-4104-eaed-49ad64fdbdb2"
      },
      "source": [
        "mask = get_attn_mask(32,4)\n",
        "plt.imshow(mask.squeeze(), cmap='gray')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd224d1cf10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L053KCrZ44SR"
      },
      "source": [
        "## Drop Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUQp43wQ8B1p",
        "outputId": "32d21113-aa08-4b00-e0cd-20feffa60eff"
      },
      "source": [
        "#(Batch_sz, num_token, embedding)\n",
        "samples = torch.ones(2,4,8)\n",
        "samples"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j8YG1Jn47CH",
        "outputId": "55d59970-b8c9-48fa-888c-c80d3285fd3a"
      },
      "source": [
        "# Zeros out an entire input sample +\n",
        "# scales up remaining samples\n",
        "drop_path = DropPath(drop_prob=0.5)\n",
        "drop_path(samples)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[2., 2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2., 2.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huBm4ORw476h"
      },
      "source": [
        "## Transformer Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9j6YuMwkwuT"
      },
      "source": [
        "class TGenerator(nn.Module):\n",
        "    def __init__(self, latent_dims=1024, img_size=32, bottom_width=8, embed_chs=384, depth=[5,2,2],\n",
        "                 drop_path_rate=0, num_heads=4, mlp_ratio=4, qkv_bias=False, qk_scale=None,  \n",
        "                 mlp_drop=0, attn_drop=0, att_mask=False, act_layer=gelu, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        assert isinstance(depth, list) and len(depth) == 3\n",
        "\n",
        "        self.in_layer = LinearReshape(in_dims=latent_dims, out_dims=embed_chs*bottom_width**2, \n",
        "                                      width=bottom_width, embed_chs=embed_chs)\n",
        "        self.pos_embed = [\n",
        "                          nn.Parameter(torch.zeros(1, bottom_width**2, embed_chs)),\n",
        "                          nn.Parameter(torch.zeros(1, (bottom_width*2)**2, embed_chs//4)),\n",
        "                          nn.Parameter(torch.zeros(1, (bottom_width*4)**2, embed_chs//16))\n",
        "        ]\n",
        "        for emb in self.pos_embed:\n",
        "            trunc_normal_(emb, std=.02)\n",
        "\n",
        "        Partial_Block = functools.partial(Block, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale, mlp_drop=mlp_drop, attn_drop=attn_drop, \n",
        "                            att_mask=att_mask, act_layer=act_layer, norm_layer=norm_layer)\n",
        "        # stochastic depth decay rule\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth[0])]  \n",
        "        self.bottom_block = nn.ModuleList([ \n",
        "                    Partial_Block(dims=embed_chs, drop_path=dpr[i]) for i in range(depth[0]) \n",
        "        ])\n",
        "        self.upsample_block = nn.ModuleList([\n",
        "                    nn.ModuleList([Partial_Block(dims=embed_chs//4, drop_path=0)] * depth[1]),\n",
        "                    nn.ModuleList([Partial_Block(dims=embed_chs//16, drop_path=0)] * depth[2])       \n",
        "        ])\n",
        "\n",
        "        self.pixel_upsample = nn.ModuleList([\n",
        "                                PixelUpsample(start_width=bottom_width),\n",
        "                                PixelUpsample(start_width=bottom_width*2)\n",
        "        ])\n",
        "        \n",
        "        self.to_rgb = To_RGB(ch_dims=embed_chs//16, img_size=img_size)\n",
        "\n",
        "    def forward(self, x, epoch=None):\n",
        "        x = self.in_layer(x) + self.pos_embed[0]\n",
        "        for blk in self.bottom_block:\n",
        "            x = blk(x,epoch)\n",
        "        for i, blocks in enumerate(self.upsample_block):\n",
        "            x = self.pixel_upsample[i](x) + self.pos_embed[i+1]\n",
        "            for blk in blocks:\n",
        "                x = blk(x,epoch)\n",
        "        x = self.to_rgb(x)\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3xUHc8hf8Ow"
      },
      "source": [
        "device = check_gpu()\n",
        "Gen = TGenerator().to(device)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "739LT0dxNp-_"
      },
      "source": [
        "# (Batch_sz,latent_dims)\n",
        "noise = torch.randn(16,1024)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vVVY_p2F8UL",
        "outputId": "7c4df370-1bc4-470f-eb5b-f5a986b9ec7c"
      },
      "source": [
        "out = Gen(noise)\n",
        "out.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPJ0SjTD5S0S",
        "outputId": "ddfe4723-4657-4965-f245-4e46cbce45fe"
      },
      "source": [
        "# Expected range [-1,1]\n",
        "out.min().item(), out.max().item()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.9978391528129578, 0.9974355101585388)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAaJQ3lr5g8p",
        "outputId": "8b19df50-d553-415c-e522-4c3812ad9347"
      },
      "source": [
        "summary(Gen,(1024,),)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                [-1, 24576]      25,165,824\n",
            "     LinearReshape-2              [-1, 64, 384]               0\n",
            "         LayerNorm-3              [-1, 64, 384]             768\n",
            "            Linear-4             [-1, 64, 1152]         442,368\n",
            "           Dropout-5            [-1, 4, 64, 64]               0\n",
            "            Linear-6              [-1, 64, 384]         147,840\n",
            "           Dropout-7              [-1, 64, 384]               0\n",
            "         Attention-8              [-1, 64, 384]               0\n",
            "          Identity-9              [-1, 64, 384]               0\n",
            "        LayerNorm-10              [-1, 64, 384]             768\n",
            "           Linear-11             [-1, 64, 1536]         591,360\n",
            "          Dropout-12             [-1, 64, 1536]               0\n",
            "           Linear-13              [-1, 64, 384]         590,208\n",
            "          Dropout-14              [-1, 64, 384]               0\n",
            "              MLP-15              [-1, 64, 384]               0\n",
            "         Identity-16              [-1, 64, 384]               0\n",
            "            Block-17              [-1, 64, 384]               0\n",
            "        LayerNorm-18              [-1, 64, 384]             768\n",
            "           Linear-19             [-1, 64, 1152]         442,368\n",
            "          Dropout-20            [-1, 4, 64, 64]               0\n",
            "           Linear-21              [-1, 64, 384]         147,840\n",
            "          Dropout-22              [-1, 64, 384]               0\n",
            "        Attention-23              [-1, 64, 384]               0\n",
            "         Identity-24              [-1, 64, 384]               0\n",
            "        LayerNorm-25              [-1, 64, 384]             768\n",
            "           Linear-26             [-1, 64, 1536]         591,360\n",
            "          Dropout-27             [-1, 64, 1536]               0\n",
            "           Linear-28              [-1, 64, 384]         590,208\n",
            "          Dropout-29              [-1, 64, 384]               0\n",
            "              MLP-30              [-1, 64, 384]               0\n",
            "         Identity-31              [-1, 64, 384]               0\n",
            "            Block-32              [-1, 64, 384]               0\n",
            "        LayerNorm-33              [-1, 64, 384]             768\n",
            "           Linear-34             [-1, 64, 1152]         442,368\n",
            "          Dropout-35            [-1, 4, 64, 64]               0\n",
            "           Linear-36              [-1, 64, 384]         147,840\n",
            "          Dropout-37              [-1, 64, 384]               0\n",
            "        Attention-38              [-1, 64, 384]               0\n",
            "         Identity-39              [-1, 64, 384]               0\n",
            "        LayerNorm-40              [-1, 64, 384]             768\n",
            "           Linear-41             [-1, 64, 1536]         591,360\n",
            "          Dropout-42             [-1, 64, 1536]               0\n",
            "           Linear-43              [-1, 64, 384]         590,208\n",
            "          Dropout-44              [-1, 64, 384]               0\n",
            "              MLP-45              [-1, 64, 384]               0\n",
            "         Identity-46              [-1, 64, 384]               0\n",
            "            Block-47              [-1, 64, 384]               0\n",
            "        LayerNorm-48              [-1, 64, 384]             768\n",
            "           Linear-49             [-1, 64, 1152]         442,368\n",
            "          Dropout-50            [-1, 4, 64, 64]               0\n",
            "           Linear-51              [-1, 64, 384]         147,840\n",
            "          Dropout-52              [-1, 64, 384]               0\n",
            "        Attention-53              [-1, 64, 384]               0\n",
            "         Identity-54              [-1, 64, 384]               0\n",
            "        LayerNorm-55              [-1, 64, 384]             768\n",
            "           Linear-56             [-1, 64, 1536]         591,360\n",
            "          Dropout-57             [-1, 64, 1536]               0\n",
            "           Linear-58              [-1, 64, 384]         590,208\n",
            "          Dropout-59              [-1, 64, 384]               0\n",
            "              MLP-60              [-1, 64, 384]               0\n",
            "         Identity-61              [-1, 64, 384]               0\n",
            "            Block-62              [-1, 64, 384]               0\n",
            "        LayerNorm-63              [-1, 64, 384]             768\n",
            "           Linear-64             [-1, 64, 1152]         442,368\n",
            "          Dropout-65            [-1, 4, 64, 64]               0\n",
            "           Linear-66              [-1, 64, 384]         147,840\n",
            "          Dropout-67              [-1, 64, 384]               0\n",
            "        Attention-68              [-1, 64, 384]               0\n",
            "         Identity-69              [-1, 64, 384]               0\n",
            "        LayerNorm-70              [-1, 64, 384]             768\n",
            "           Linear-71             [-1, 64, 1536]         591,360\n",
            "          Dropout-72             [-1, 64, 1536]               0\n",
            "           Linear-73              [-1, 64, 384]         590,208\n",
            "          Dropout-74              [-1, 64, 384]               0\n",
            "              MLP-75              [-1, 64, 384]               0\n",
            "         Identity-76              [-1, 64, 384]               0\n",
            "            Block-77              [-1, 64, 384]               0\n",
            "     PixelShuffle-78           [-1, 96, 16, 16]               0\n",
            "    PixelUpsample-79              [-1, 256, 96]               0\n",
            "        LayerNorm-80              [-1, 256, 96]             192\n",
            "           Linear-81             [-1, 256, 288]          27,648\n",
            "          Dropout-82          [-1, 4, 256, 256]               0\n",
            "           Linear-83              [-1, 256, 96]           9,312\n",
            "          Dropout-84              [-1, 256, 96]               0\n",
            "        Attention-85              [-1, 256, 96]               0\n",
            "         Identity-86              [-1, 256, 96]               0\n",
            "        LayerNorm-87              [-1, 256, 96]             192\n",
            "           Linear-88             [-1, 256, 384]          37,248\n",
            "          Dropout-89             [-1, 256, 384]               0\n",
            "           Linear-90              [-1, 256, 96]          36,960\n",
            "          Dropout-91              [-1, 256, 96]               0\n",
            "              MLP-92              [-1, 256, 96]               0\n",
            "         Identity-93              [-1, 256, 96]               0\n",
            "            Block-94              [-1, 256, 96]               0\n",
            "        LayerNorm-95              [-1, 256, 96]             192\n",
            "           Linear-96             [-1, 256, 288]          27,648\n",
            "          Dropout-97          [-1, 4, 256, 256]               0\n",
            "           Linear-98              [-1, 256, 96]           9,312\n",
            "          Dropout-99              [-1, 256, 96]               0\n",
            "       Attention-100              [-1, 256, 96]               0\n",
            "        Identity-101              [-1, 256, 96]               0\n",
            "       LayerNorm-102              [-1, 256, 96]             192\n",
            "          Linear-103             [-1, 256, 384]          37,248\n",
            "         Dropout-104             [-1, 256, 384]               0\n",
            "          Linear-105              [-1, 256, 96]          36,960\n",
            "         Dropout-106              [-1, 256, 96]               0\n",
            "             MLP-107              [-1, 256, 96]               0\n",
            "        Identity-108              [-1, 256, 96]               0\n",
            "           Block-109              [-1, 256, 96]               0\n",
            "    PixelShuffle-110           [-1, 24, 32, 32]               0\n",
            "   PixelUpsample-111             [-1, 1024, 24]               0\n",
            "       LayerNorm-112             [-1, 1024, 24]              48\n",
            "          Linear-113             [-1, 1024, 72]           1,728\n",
            "         Dropout-114        [-1, 4, 1024, 1024]               0\n",
            "          Linear-115             [-1, 1024, 24]             600\n",
            "         Dropout-116             [-1, 1024, 24]               0\n",
            "       Attention-117             [-1, 1024, 24]               0\n",
            "        Identity-118             [-1, 1024, 24]               0\n",
            "       LayerNorm-119             [-1, 1024, 24]              48\n",
            "          Linear-120             [-1, 1024, 96]           2,400\n",
            "         Dropout-121             [-1, 1024, 96]               0\n",
            "          Linear-122             [-1, 1024, 24]           2,328\n",
            "         Dropout-123             [-1, 1024, 24]               0\n",
            "             MLP-124             [-1, 1024, 24]               0\n",
            "        Identity-125             [-1, 1024, 24]               0\n",
            "           Block-126             [-1, 1024, 24]               0\n",
            "       LayerNorm-127             [-1, 1024, 24]              48\n",
            "          Linear-128             [-1, 1024, 72]           1,728\n",
            "         Dropout-129        [-1, 4, 1024, 1024]               0\n",
            "          Linear-130             [-1, 1024, 24]             600\n",
            "         Dropout-131             [-1, 1024, 24]               0\n",
            "       Attention-132             [-1, 1024, 24]               0\n",
            "        Identity-133             [-1, 1024, 24]               0\n",
            "       LayerNorm-134             [-1, 1024, 24]              48\n",
            "          Linear-135             [-1, 1024, 96]           2,400\n",
            "         Dropout-136             [-1, 1024, 96]               0\n",
            "          Linear-137             [-1, 1024, 24]           2,328\n",
            "         Dropout-138             [-1, 1024, 24]               0\n",
            "             MLP-139             [-1, 1024, 24]               0\n",
            "        Identity-140             [-1, 1024, 24]               0\n",
            "           Block-141             [-1, 1024, 24]               0\n",
            "          Linear-142              [-1, 1024, 3]              75\n",
            "            Tanh-143              [-1, 1024, 3]               0\n",
            "          To_RGB-144            [-1, 3, 32, 32]               0\n",
            "================================================================\n",
            "Total params: 34,269,867\n",
            "Trainable params: 34,269,867\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 106.95\n",
            "Params size (MB): 130.73\n",
            "Estimated Total Size (MB): 237.68\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft5JF3vk6QIm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}